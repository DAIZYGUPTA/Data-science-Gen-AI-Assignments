{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***CNN Architecture Assignment ***"
      ],
      "metadata": {
        "id": "Y4rzfH8aZd-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN0mLbS6ZbdR"
      },
      "outputs": [],
      "source": [
        "#Q1. What is a Convolutional Neural Network (CNN), and why is it used for image processing\n",
        "\n",
        "\"\"\"\n",
        "A Convolutional Neural Network (CNN) is a deep learning architecture designed for\n",
        "image processing and pattern recognition. It uses convolutional layers to extract\n",
        "spatial features from images, making it highly effective for tasks like\n",
        "image classification, object detection, and segmentation.\n",
        "\n",
        "Why is CNN Used for Image Processing?\n",
        "Feature Extraction: CNN automatically detects edges, textures, and patterns.\n",
        "Spatial Hierarchy: Captures local and global image features effectively.\n",
        "Parameter Efficiency: Uses convolutional filters instead of fully connected layers, reducing computations.\n",
        "Translation Invariance: Detects objects regardless of position in the image.\n",
        "Deep Learning Power: Enables end-to-end learning for complex visual tasks.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. What are the key components of a CNN architecture\n",
        "\n",
        "\"\"\"\n",
        "Convolutional Layers\n",
        "\n",
        "Extract features (edges, textures, shapes) using filters/kernels.\n",
        "Uses stride and padding to control feature map size.\n",
        "Activation function (e.g., ReLU) is applied for non-linearity.\n",
        "Pooling Layers\n",
        "\n",
        "Reduce spatial dimensions, making computation efficient.\n",
        "Max Pooling (most common) selects the highest value in a region.\n",
        "Fully Connected (FC) Layers\n",
        "\n",
        "Flatten feature maps and connect to output neurons.\n",
        "Used for classification or regression tasks.\n",
        "Dropout (Regularization)\n",
        "\n",
        "Prevents overfitting by randomly deactivating neurons during training.\n",
        "Softmax / Sigmoid Layer\n",
        "\n",
        "Converts final layer outputs into probabilities for classification.\n",
        "Softmax (multi-class), Sigmoid (binary classification).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ct4FxSdpZkS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. What is the role of the convolutional layer in CNNs\n",
        "\n",
        "\"\"\"\n",
        "The convolutional layer is the core building block of a Convolutional Neural Network (CNN).\n",
        "It extracts features from an input image by applying filters (kernels) that detect edges, textures, and patterns.\n",
        "\n",
        "How It Works:\n",
        "Filters/Kernels: Small matrices (e.g., 3×3 or 5×5) slide over the input image.\n",
        "Dot Product: Each filter performs a dot product with pixel values to produce a feature map.\n",
        "Feature Detection: Lower layers detect edges & textures, deeper layers detect complex objects.\n",
        "Non-Linearity (ReLU): Activates important features while ignoring irrelevant ones.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sHThqJylZkPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. What is a filter (kernel) in CNNs\n",
        "\n",
        "\"\"\"\n",
        "A filter (kernel) in a Convolutional Neural Network (CNN) is a small matrix (e.g., 3×3, 5×5)\n",
        "that slides over an image to detect features like edges, textures, and patterns.\n",
        "\n",
        "How It Works:\n",
        "Sliding Window: The filter moves across the image, performing a dot product with pixel values.\n",
        "Feature Extraction: Different filters detect horizontal, vertical, diagonal edges, textures, and complex patterns.\n",
        "Multiple Filters: Each convolutional layer uses multiple filters to extract diverse features.\n",
        "Activation Function (ReLU): Introduces non-linearity to retain important features.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Sl5MUX6pZo0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. What is pooling in CNNs, and why is it important\n",
        "\n",
        "\"\"\"\n",
        "Pooling is a downsampling technique in Convolutional Neural Networks (CNNs)\n",
        "that reduces the spatial dimensions of feature maps while preserving essential information.\n",
        "\n",
        "Types of Pooling:\n",
        "Max Pooling (Most Common)\n",
        "\n",
        "Selects the maximum value from a region (e.g., 2×2 window).\n",
        "Helps retain the strongest features.\n",
        "Average Pooling\n",
        "\n",
        "Computes the average value from a region.\n",
        "Used when smooth feature extraction is preferred.\n",
        "Why is Pooling Important?\n",
        " Reduces Computation: Fewer parameters, faster training.\n",
        " Prevents Overfitting: Removes unnecessary details.\n",
        " Translation Invariance: Detects patterns regardless of position.\n",
        " Enhances Generalization: Helps CNNs recognize objects in different scales.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WNY1Vy5eZoyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. What are the common types of pooling used in CNNs\n",
        "\n",
        "\"\"\"\n",
        "1. Max Pooling (Most Common)\n",
        "Selects the maximum value in a window (e.g., 2×2 or 3×3).\n",
        "Retains the most prominent features (e.g., edges, textures).\n",
        "2. Average Pooling\n",
        "Takes the average value of pixels in a region.\n",
        "Smooths the feature map but may lose sharp details.\n",
        "Used in: Feature reduction in classification models.\n",
        "3. Global Pooling\n",
        "Computes a single value per feature map.\n",
        "Types:\n",
        "Global Max Pooling: Takes the max value of the entire feature map.\n",
        "Global Average Pooling: Takes the average value of the entire feature map.\n",
        "Used in: Reducing feature maps to a single vector before fully connected layers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YRhmtsGBZowc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Max Pooling\n",
        "Input:  [[1, 3],\n",
        "         [5, 2]]\n",
        "Max Pooling → 5 (highest value)\n"
      ],
      "metadata": {
        "id": "iw0SWMDRbJF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. How does the backpropagation algorithm work in CNNs"
      ],
      "metadata": {
        "id": "1eljNf_VZouO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How Backpropagation Works in CNNs**\n",
        "\n",
        "Backpropagation is the **learning process** in CNNs, where the model updates its **weights** by minimizing the **loss function** using **gradient descent**.\n",
        "\n",
        "## **Steps in Backpropagation**\n",
        "\n",
        "### **1. Forward Pass**\n",
        "- The input image passes through **convolutional, activation, pooling, and fully connected layers**.\n",
        "- The model generates a **prediction**.\n",
        "\n",
        "### **2. Loss Calculation**\n",
        "- The difference between the predicted output and actual label is calculated using a **loss function** (e.g., CrossEntropyLoss).\n",
        "\n",
        "### **3. Backward Pass (Gradient Computation)**\n",
        "- Compute **gradients** of the loss with respect to weights using the **chain rule**.\n",
        "- **Errors are propagated backward** from the output layer to convolutional layers.\n",
        "\n",
        "### **4. Weight Update (Gradient Descent)**\n",
        "- Weights are updated using an **optimizer** (e.g., SGD, Adam).\n",
        "- Update formula:  \n",
        "  \\[\n",
        "  W = W - \\eta \\frac{\\partial L}{\\partial W}\n",
        "  \\]\n",
        "  where **\\( \\eta \\)** is the learning rate and **\\( L \\)** is the loss.\n",
        "\n",
        "### **5. Repeat Until Convergence**\n",
        "- The process repeats for multiple **epochs** until the model reaches optimal accuracy.\n",
        "\n",
        " **Backpropagation enables CNNs to learn patterns and improve accuracy over time!**\n"
      ],
      "metadata": {
        "id": "8dH97QancIZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. What is the role of activation functions in CNNs"
      ],
      "metadata": {
        "id": "9aT9UL60ZosM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Role of Activation Functions in CNNs**\n",
        "\n",
        "Activation functions introduce **non-linearity** in CNNs, helping the model learn **complex patterns** beyond simple linear transformations.\n",
        "\n",
        "## **Why Are Activation Functions Important?**\n",
        " Enable **non-linearity** for capturing complex features.  \n",
        " Prevent **vanishing gradients**, improving deep network training.  \n",
        " Improve **model performance** by deciding neuron activation.\n",
        "\n",
        "## **Common Activation Functions in CNNs**\n",
        "### **1. ReLU (Rectified Linear Unit) **\n",
        "- **Formula:** \\( f(x) = \\max(0, x) \\)  \n",
        "- Prevents **vanishing gradients** and speeds up training.  \n",
        "- Most commonly used in **hidden layers**.\n",
        "\n",
        "### **2. Sigmoid **\n",
        "- **Formula:** \\( f(x) = \\frac{1}{1 + e^{-x}} \\)  \n",
        "- Produces values **between 0 and 1** (good for probabilities).  \n",
        "- Not ideal for deep CNNs due to **vanishing gradients**.\n",
        "\n",
        "### **3. Tanh **\n",
        "- **Formula:** \\( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)  \n",
        "- Output ranges from **-1 to 1** (zero-centered).  \n",
        "- Still suffers from **vanishing gradients**.\n",
        "\n",
        "### **4. Softmax **\n",
        "- Used in the **final layer** for **multi-class classification**.  \n",
        "- Converts logits into **probabilities** summing to **1**.\n",
        "\n",
        " **ReLU is the most widely used activation function in CNNs for its efficiency and performance!**\n"
      ],
      "metadata": {
        "id": "q-K-1rKVbyPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. What is the concept of receptive fields in CNNs"
      ],
      "metadata": {
        "id": "PDeSOQbAZop0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Concept of Receptive Fields in CNNs**\n",
        "\n",
        "The **receptive field** in a CNN is the **region of the input image** that influences a particular neuron in a layer. It defines how much of the original image a neuron **\"sees\"**.\n",
        "\n",
        "## **Key Points About Receptive Fields**\n",
        " **Local Connectivity:** In convolutional layers, each neuron only looks at a small portion of the image.  \n",
        " **Increases with Depth:** The receptive field grows as we move deeper in the network.  \n",
        " **Captures Hierarchical Features:** Early layers detect edges, while deeper layers recognize complex objects.  \n",
        "\n",
        "## **How Receptive Field Expands**\n",
        "- A **single filter (e.g., 3×3)** in the first layer has a **small receptive field**.  \n",
        "- As we stack **multiple layers**, each neuron **combines information** from a larger portion of the image.  \n",
        "- **Pooling layers** further expand the receptive field by reducing spatial dimensions.\n",
        "\n",
        " **Larger receptive fields help CNNs recognize high-level patterns, making them effective in deep learning tasks!**\n"
      ],
      "metadata": {
        "id": "6rxKZqpqcXvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10.  Explain the concept of tensor space in CNNs"
      ],
      "metadata": {
        "id": "md1lInb3Zonb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Concept of Tensor Space in CNNs**\n",
        "\n",
        "A **tensor space** in CNNs represents the **multi-dimensional data structure** used to store and process images, feature maps, and weights.\n",
        "\n",
        "## **Key Aspects of Tensor Space**\n",
        " **Tensors are Multi-Dimensional Arrays:** CNNs operate on 4D tensors representing images as **(Batch, Height, Width, Channels)**.  \n",
        " **Efficient Computation:** Tensor operations enable parallel computation, optimizing performance.  \n",
        " **Feature Transformation:** Convolution and pooling layers modify tensor dimensions while preserving spatial relationships.  \n",
        "\n",
        "## **Example of Tensor Representation in CNNs**\n",
        "- **Input Image Tensor:** \\( (1, 224, 224, 3) \\) → Single 224×224 RGB image.  \n",
        "- **Feature Map Tensor (After Conv Layer):** \\( (1, 112, 112, 64) \\) → 64 feature maps after downsampling.  \n",
        "- **Fully Connected Layer Tensor:** \\( (1, 1, 1, 1000) \\) → 1000 neurons for classification.\n",
        "\n",
        " **Tensor space enables CNNs to efficiently process high-dimensional data for deep learning!**\n"
      ],
      "metadata": {
        "id": "RAh6ivY2cqNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11. What is LeNet-5, and how does it contribute to the development of CNNs"
      ],
      "metadata": {
        "id": "yT97HBAeZolG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LeNet-5 and Its Contribution to CNN Development**\n",
        "\n",
        "LeNet-5 is one of the **earliest Convolutional Neural Networks (CNNs)**, developed by **Yann LeCun in 1998** for **handwritten digit recognition (MNIST dataset)**.\n",
        "\n",
        "## **Architecture of LeNet-5**\n",
        "1️ **Input Layer** → \\( 32 \\times 32 \\) grayscale image.  \n",
        "2️ **Conv Layer 1 (C1):** 6 filters of size \\( 5 \\times 5 \\), output \\( 28 \\times 28 \\times 6 \\).  \n",
        "3️ **Pooling Layer 1 (S2):** Avg pooling, output \\( 14 \\times 14 \\times 6 \\).  \n",
        "4️ **Conv Layer 2 (C3):** 16 filters of size \\( 5 \\times 5 \\), output \\( 10 \\times 10 \\times 16 \\).  \n",
        "5️ **Pooling Layer 2 (S4):** Avg pooling, output \\( 5 \\times 5 \\times 16 \\).  \n",
        "6️ **Fully Connected Layer (C5):** Flattened to 120 neurons.  \n",
        "7️ **FC Layer (F6):** 84 neurons.  \n",
        "8️ **Output Layer:** 10 neurons (for digit classification).\n",
        "\n",
        "## **Contribution to CNN Development**\n",
        " **Introduced Core CNN Concepts:** Convolution, pooling, and fully connected layers.  \n",
        " **Inspired Modern Architectures:** Basis for networks like AlexNet, VGG, and ResNet.  \n",
        " **Efficient Feature Extraction:** Reduced need for manual feature engineering.\n",
        "\n",
        " **LeNet-5 laid the foundation for deep learning in image processing and recognition!**\n"
      ],
      "metadata": {
        "id": "6YUS_jUbc2Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12. What is AlexNet, and why was it a breakthrough in deep learning"
      ],
      "metadata": {
        "id": "3HM1PkxkZoi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AlexNet: A Breakthrough in Deep Learning**\n",
        "\n",
        "AlexNet, developed by **Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012**, was a **deep CNN** that won the **ImageNet competition** by a large margin, marking a breakthrough in deep learning.\n",
        "\n",
        "## **Key Features of AlexNet**\n",
        "1️ **Deep Architecture:** 8 layers (5 convolutional + 3 fully connected).  \n",
        "2️ **ReLU Activation:** Faster training compared to sigmoid/tanh.  \n",
        "3️ **Dropout Regularization:** Prevents overfitting.  \n",
        "4️ **Data Augmentation:** Used image transformations to enhance training.  \n",
        "5️ **GPU Acceleration:** First CNN to leverage **parallel processing** using GPUs.\n",
        "\n",
        "## **Why AlexNet Was a Breakthrough**\n",
        " **Significantly Reduced Error Rate** on ImageNet (from ~26% to 15%).  \n",
        " **Popularized Deep Learning for Vision Tasks.**  \n",
        " **Led to More Advanced Architectures** (VGG, ResNet, etc.).  \n",
        "\n",
        " **AlexNet revolutionized computer vision, making deep CNNs the standard for image processing!**\n"
      ],
      "metadata": {
        "id": "bsGrbRHydFhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13. What is VGGNet, and how does it differ from AlexNet"
      ],
      "metadata": {
        "id": "K-8_52kHZogs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VGGNet and Its Differences from AlexNet**\n",
        "\n",
        "VGGNet (Visual Geometry Group Network), developed by **Simonyan and Zisserman in 2014**, is a **deep CNN** that improved upon AlexNet by using **smaller 3×3 filters** and a **deeper architecture**.\n",
        "\n",
        "## **Key Features of VGGNet**\n",
        "1️ **Deeper Architecture:** 16 or 19 layers (VGG-16, VGG-19) vs. **8 layers in AlexNet**.  \n",
        "2️ **Small 3×3 Convolutions:** Instead of larger filters (e.g., 11×11 in AlexNet), improving feature extraction.  \n",
        "3️ **More Feature Maps:** Up to **512 channels** in deeper layers.  \n",
        "4️ **Consistent Architecture:** Only **3×3 conv + 2×2 pooling** layers used repeatedly.  \n",
        "5️ **Fully Connected Layers:** Same as AlexNet (4096 neurons).\n",
        "\n",
        "## **Differences Between VGGNet and AlexNet**\n",
        "| Feature         | AlexNet            | VGGNet (VGG-16)     |\n",
        "|---------------|-----------------|----------------|\n",
        "| **Depth**    | 8 layers         | 16 layers      |\n",
        "| **Conv Filters** | 11×11, 5×5, 3×3  | 3×3 (only)     |\n",
        "| **Pooling**  | 3×3 max pooling  | 2×2 max pooling |\n",
        "| **Feature Maps** | Up to 256       | Up to 512      |\n",
        "\n",
        " **VGGNet improved accuracy with deeper networks and uniform architecture, influencing later models like ResNet!**\n"
      ],
      "metadata": {
        "id": "kyRJjY6MdScy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14. What is GoogLeNet, and what is its main innovation"
      ],
      "metadata": {
        "id": "sPKYfOfPZoeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GoogLeNet and Its Main Innovation**\n",
        "\n",
        "GoogLeNet, developed by **Google in 2014**, won the **ImageNet Challenge** and introduced the **Inception architecture**, making CNNs more **efficient and deeper** without increasing computational cost.\n",
        "\n",
        "## **Main Innovation: Inception Module**\n",
        "1️ **Multi-scale Feature Extraction:** Uses **1×1, 3×3, and 5×5 convolutions** in parallel to capture different spatial patterns.  \n",
        "2️ **1×1 Convolutions (Dimensionality Reduction):** Reduces the number of parameters, making the network computationally efficient.  \n",
        "3️ **Deeper but Efficient Architecture:** **22 layers** but **fewer parameters (5M vs. 60M in VGG-16)** due to smart filter selection.  \n",
        "4️ **Global Average Pooling (GAP):** Replaces fully connected layers, reducing overfitting and improving generalization.\n",
        "\n",
        "## **Impact of GoogLeNet**\n",
        " **Deeper CNNs Without Excessive Parameters.**  \n",
        " **Inspired Later Models** (ResNet, Inception-V3, EfficientNet).  \n",
        " **Proved That Depth + Efficiency Leads to Better Performance.**  \n",
        "\n",
        " **GoogLeNet revolutionized deep learning by introducing modular architectures like Inception!**\n"
      ],
      "metadata": {
        "id": "5hZGabBjdXAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15. What is ResNet, and what problem does it solve"
      ],
      "metadata": {
        "id": "qWvah77QZob8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ResNet and the Problem It Solves**\n",
        "\n",
        "ResNet (Residual Network), developed by **Microsoft in 2015**, won the **ImageNet Challenge** by introducing **residual learning**, allowing CNNs to be **extremely deep (up to 152 layers)** without suffering from the **vanishing gradient problem**.\n",
        "\n",
        "## **Problem ResNet Solves: Vanishing Gradient**\n",
        " **Deep networks suffer from vanishing gradients**, making training difficult.  \n",
        " **Traditional deep CNNs** (e.g., VGG) struggle to learn beyond a certain depth.  \n",
        "\n",
        "## **Key Innovation: Residual Connections**\n",
        " **Skip Connections (Shortcut Paths):** Allows gradients to flow directly, bypassing layers and enabling very deep networks.  \n",
        " **Residual Learning:** Instead of learning the output **directly**, the network learns a **residual function**:  \n",
        "   \\[\n",
        "   y = F(x) + x\n",
        "   \\]\n",
        " **Easier Optimization:** Helps train deeper models **without degradation** in accuracy.\n",
        "\n",
        "## **Impact of ResNet**\n",
        "✔ **Enabled Extremely Deep Networks (ResNet-50, ResNet-101, ResNet-152).**  \n",
        "✔ **Improved Accuracy Without Extra Complexity.**  \n",
        "✔ **Inspired Advanced Architectures (DenseNet, EfficientNet).**  \n",
        "\n",
        " **ResNet solved the vanishing gradient problem, making deep learning scalable and more effective!**\n"
      ],
      "metadata": {
        "id": "V1bEjRdkdphE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q16. What is DenseNet, and how does it differ from ResNet"
      ],
      "metadata": {
        "id": "8Y0ioGV_Z4ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DenseNet and Its Difference from ResNet**\n",
        "\n",
        "DenseNet (Densely Connected Network), introduced in **2017**, improves deep learning by using **dense connections** between layers, ensuring maximum feature reuse and efficient gradient flow.\n",
        "\n",
        "## **Key Features of DenseNet**\n",
        "1️ **Dense Connectivity:** Each layer receives **inputs from all previous layers** and passes its output to **all subsequent layers**.  \n",
        "2️ **Feature Reuse:** Unlike ResNet, which adds **residuals (y = F(x) + x)**, DenseNet **concatenates** previous outputs:  \n",
        "   \\[\n",
        "   x_{l} = [x_{0}, x_{1}, ..., x_{l-1}]\n",
        "   \\]  \n",
        "3️ **Fewer Parameters:** Avoids redundant feature maps, making it more **parameter-efficient** than ResNet.  \n",
        "4️ **Better Gradient Flow:** Direct connections help prevent the **vanishing gradient problem**, similar to ResNet.  \n",
        "\n",
        "## **Differences Between DenseNet and ResNet**\n",
        "| Feature         | ResNet             | DenseNet          |\n",
        "|---------------|-----------------|----------------|\n",
        "| **Connections**  | Skip (residual) connections | Direct dense connections |\n",
        "| **Feature Passing** | Adds residuals \\( y = F(x) + x \\) | Concatenates features \\( [x_0, x_1, ..., x_l] \\) |\n",
        "| **Parameter Efficiency** | Higher | Lower (fewer redundant features) |\n",
        "| **Gradient Flow** | Good | Even better |\n",
        "\n",
        " **DenseNet improves learning efficiency, reduces parameters, and enhances feature propagation compared to ResNet!**\n"
      ],
      "metadata": {
        "id": "gES4QMVLd4gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17. What are the main steps involved in training a CNN from scratch"
      ],
      "metadata": {
        "id": "6w1FZzbmZ4BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Steps in Training a CNN from Scratch**\n",
        "\n",
        "1️ **Data Collection & Preprocessing**  \n",
        "   - Gather labeled images for training.  \n",
        "   - Normalize pixel values (e.g., scale between 0 and 1).  \n",
        "   - Augment data (rotation, flipping, cropping) to improve generalization.  \n",
        "\n",
        "2️ **Build the CNN Model**  \n",
        "   - Define **Convolutional Layers** to extract features.  \n",
        "   - Add **Pooling Layers** to reduce spatial dimensions.  \n",
        "   - Use **Fully Connected Layers** for classification.  \n",
        "   - Choose an **activation function** (e.g., ReLU for hidden layers, Softmax for output).  \n",
        "\n",
        "3️ **Define Loss Function & Optimizer**  \n",
        "   - **Loss Function:** Cross-Entropy (for classification) or MSE (for regression).  \n",
        "   - **Optimizer:** Adam, SGD, or RMSprop to update weights.  \n",
        "\n",
        "4️ **Train the Model**  \n",
        "   - Feed images in **mini-batches**.  \n",
        "   - Perform **forward propagation** to compute predictions.  \n",
        "   - Use **backpropagation** to update weights via gradient descent.  \n",
        "   - Repeat for multiple **epochs**.  \n",
        "\n",
        "5️ **Evaluate the Model**  \n",
        "   - Use **validation data** to check accuracy and loss.  \n",
        "   - Plot **training vs validation loss** to detect overfitting.  \n",
        "\n",
        "6️ **Fine-Tuning & Optimization**  \n",
        "   - Adjust **learning rate**, batch size, or layers if needed.  \n",
        "   - Use **dropout or L2 regularization** to prevent overfitting.  \n",
        "\n",
        "7️ **Test & Deploy**  \n",
        "   - Evaluate on **unseen test data**.  \n",
        "   - Convert model to **deployable format** (e.g., TensorFlow SavedModel, ONNX).  \n",
        "\n",
        " **Following these steps ensures an effective CNN model for image classification or other vision tasks!**\n"
      ],
      "metadata": {
        "id": "y-AYW5Z0eHlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL**"
      ],
      "metadata": {
        "id": "uuS9lGASeHU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1.  Implement a basic convolution operation using a filter and a 5x5 image (matrix)"
      ],
      "metadata": {
        "id": "rOc8dUXZZ3--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "# Define a 5x5 image (matrix)\n",
        "image = np.array([\n",
        "    [10, 20, 30, 40, 50],\n",
        "    [60, 70, 80, 90, 100],\n",
        "    [110, 120, 130, 140, 150],\n",
        "    [160, 170, 180, 190, 200],\n",
        "    [210, 220, 230, 240, 250]\n",
        "])\n",
        "\n",
        "# Define a 3x3 filter (kernel)\n",
        "kernel = np.array([\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1]\n",
        "])\n",
        "\n",
        "# Perform the convolution operation\n",
        "convolved_output = convolve2d(image, kernel, mode='valid')  # 'valid' ensures no padding\n",
        "\n",
        "# Display the result\n",
        "print(\"Convolved Output:\\n\", convolved_output)\n"
      ],
      "metadata": {
        "id": "tHAD2CNHe5nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.  Implement max pooling on a 4x4 feature map with a 2x2 window"
      ],
      "metadata": {
        "id": "02Y36S-5Z387"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a 4x4 feature map\n",
        "feature_map = np.array([\n",
        "    [1, 3, 2, 1],\n",
        "    [4, 6, 5, 7],\n",
        "    [8, 9, 10, 12],\n",
        "    [13, 15, 14, 16]\n",
        "])\n",
        "\n",
        "# Define pooling window size\n",
        "pool_size = 2\n",
        "\n",
        "# Get output shape\n",
        "output_shape = (feature_map.shape[0] // pool_size, feature_map.shape[1] // pool_size)\n",
        "pooled_output = np.zeros(output_shape)\n",
        "\n",
        "# Perform max pooling\n",
        "for i in range(0, feature_map.shape[0], pool_size):\n",
        "    for j in range(0, feature_map.shape[1], pool_size):\n",
        "        pooled_output[i//pool_size, j//pool_size] = np.max(feature_map[i:i+pool_size, j:j+pool_size])\n",
        "\n",
        "# Display the result\n",
        "print(\"Max Pooled Output:\\n\", pooled_output)\n"
      ],
      "metadata": {
        "id": "9tBf0ePVe9Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. Implement the ReLU activation function on a feature map"
      ],
      "metadata": {
        "id": "dILCmRL_Z36h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a feature map with positive and negative values\n",
        "feature_map = np.array([\n",
        "    [-2, 3, -1, 5],\n",
        "    [4, -6, 0, 7],\n",
        "    [-3, 8, -9, 2],\n",
        "    [6, -4, 1, -7]\n",
        "])\n",
        "\n",
        "# Apply ReLU activation function\n",
        "relu_output = np.maximum(0, feature_map)\n",
        "\n",
        "# Display the result\n",
        "print(\"ReLU Activated Output:\\n\", relu_output)\n"
      ],
      "metadata": {
        "id": "CzLP7dLbZ34a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. Create a simple CNN model with one convolutional layer and a fully connected layer, using random data"
      ],
      "metadata": {
        "id": "n8eg3PiBZ32B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Generate random input data (10 samples of 28x28 grayscale images)\n",
        "X_train = np.random.rand(10, 28, 28, 1).astype(np.float32)\n",
        "y_train = np.random.randint(0, 10, 10)  # 10 random labels (for a 10-class problem)\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),  # Conv layer\n",
        "    layers.MaxPooling2D((2,2)),  # Pooling layer\n",
        "    layers.Flatten(),  # Flatten feature maps\n",
        "    layers.Dense(10, activation='softmax')  # Fully connected output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model on random data\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=2)\n"
      ],
      "metadata": {
        "id": "eX3S27YhZ3zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. Generate a synthetic dataset using random noise and train a simple CNN model on it"
      ],
      "metadata": {
        "id": "bns1Jp7NZ3w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset (random noise images)\n",
        "num_samples = 1000\n",
        "image_size = (28, 28, 1)  # Grayscale images\n",
        "num_classes = 10\n",
        "\n",
        "X_train = np.random.rand(num_samples, *image_size).astype(np.float32)  # Random noise images\n",
        "y_train = np.random.randint(0, num_classes, num_samples)  # Random labels\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3,3), activation='relu', input_shape=image_size),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model on the synthetic dataset\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "id": "8F5A9S4pZ3up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. Create a simple CNN using Keras with one convolution layer and a max-pooling layer"
      ],
      "metadata": {
        "id": "it8rorE2Z3sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional Layer\n",
        "    layers.MaxPooling2D((2,2))  # Max-Pooling Layer\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "xmEkfh8fZ3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Write a code to add a fully connected layer after the convolution and max-pooling layers in a CNN"
      ],
      "metadata": {
        "id": "Eo1W5UBKZ3na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define a CNN model with a fully connected layer\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional Layer\n",
        "    layers.MaxPooling2D((2,2)),  # Max-Pooling Layer\n",
        "    layers.Flatten(),  # Flatten feature maps\n",
        "    layers.Dense(64, activation='relu'),  # Fully Connected Layer\n",
        "    layers.Dense(10, activation='softmax')  # Output Layer (10 classes)\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "vImeRUahZ3lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Write a code to add  batch normalization to a simple CNN model"
      ],
      "metadata": {
        "id": "j4yHjtahZ3iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define a CNN model with batch normalization\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional Layer\n",
        "    layers.BatchNormalization(),  # Batch Normalization\n",
        "    layers.MaxPooling2D((2,2)),  # Max-Pooling Layer\n",
        "    layers.Flatten(),  # Flatten feature maps\n",
        "    layers.Dense(64, activation='relu'),  # Fully Connected Layer\n",
        "    layers.BatchNormalization(),  # Batch Normalization\n",
        "    layers.Dense(10, activation='softmax')  # Output Layer (10 classes)\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "-jUguvAgZ3gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. Write a code to add dropout regularization to a simple CNN mode|"
      ],
      "metadata": {
        "id": "7fXu52ebZ3d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define a CNN model with dropout regularization\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional Layer\n",
        "    layers.MaxPooling2D((2,2)),  # Max-Pooling Layer\n",
        "    layers.Dropout(0.25),  # Dropout Regularization\n",
        "    layers.Flatten(),  # Flatten feature maps\n",
        "    layers.Dense(64, activation='relu'),  # Fully Connected Layer\n",
        "    layers.Dropout(0.5),  # Dropout Regularization\n",
        "    layers.Dense(10, activation='softmax')  # Output Layer (10 classes)\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "v-pUoVAYf1UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a code to print the architecture of the VGG16 model in Keras"
      ],
      "metadata": {
        "id": "zunow0Olf1Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the VGG16 model\n",
        "model = VGG16()\n",
        "\n",
        "# Print model architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "bQr5Nb_Ff1Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11. Write a code to plot the accuracy and loss graphs after training a CNN model"
      ],
      "metadata": {
        "id": "yCbQTAOif6q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'history' is the object returned by model.fit()\n",
        "def plot_training_history(history):\n",
        "    # Extract accuracy and loss values\n",
        "    acc = history.history['accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_acc = history.history.get('val_accuracy')  # For validation accuracy (if available)\n",
        "    val_loss = history.history.get('val_loss')  # For validation loss (if available)\n",
        "\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
        "    if val_acc: plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "    if val_loss: plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage after training:\n",
        "# history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "# plot_training_history(history)\n"
      ],
      "metadata": {
        "id": "zxV1muB5f6uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12. Write a code to print the architecture of the ResNet50 model in Keras"
      ],
      "metadata": {
        "id": "FwlsYRN0f60D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the ResNet50 model\n",
        "model = ResNet50()\n",
        "\n",
        "# Print model architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "6JqdLrTNgh8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13. Write a code to train a basic CNN model and print the training loss and accuracy after each epoch"
      ],
      "metadata": {
        "id": "7KHzKT-Dghnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load and preprocess dataset (using MNIST for example)\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Define a basic CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and print loss & accuracy after each epoch\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n"
      ],
      "metadata": {
        "id": "DCAzHE1jghgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}