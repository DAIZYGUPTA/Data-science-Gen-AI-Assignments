{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Deep Learning Frameworks Assignment ***"
      ],
      "metadata": {
        "id": "mi6AuyzHOPrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP5giDFqOJpY"
      },
      "outputs": [],
      "source": [
        "#Q1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x\n",
        "\n",
        "\"\"\"\n",
        "TensorFlow 2.0 is a major upgrade from TensorFlow 1.x, designed to be more user-friendly, flexible, and efficient. Key differences include:\n",
        "\n",
        "Eager Execution by Default â€“ No need to define static graphs first; computations run immediately.\n",
        "Simplified API â€“ tf.keras is now the primary high-level API for building models.\n",
        "Better Performance â€“ Optimized execution with tf.function for automatic graph conversion.\n",
        "Simpler Data Pipelines â€“ tf.data makes handling datasets easier.\n",
        "Unified RNNs and Distribution Strategy â€“ Easier multi-GPU and TPU training.\n",
        "Dropped Redundant Features â€“ No more tf.Session(), tf.placeholder(), or tf.global_variables_initializer().\n",
        "Overall, TensorFlow 2.0 is more Pythonic, intuitive, and powerful for deep learning tasks\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.  How do you install TensorFlow 2.0\n",
        "\n",
        "# Install TensorFlow 2.0\n",
        "pip install tensorflow\n",
        "\n",
        "# Install a Specific Version (e.g., TensorFlow 2.0.0)\n",
        "pip install tensorflow==2.0.0\n",
        "\n",
        "\n",
        "# Verify Installation\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "id": "yf9yYg-XObIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. What is the primary function of the tf.function in TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "The primary function of tf.function in TensorFlow 2.0 is to convert a Python function into a TensorFlow computation graph, optimizing execution for performance and efficiency.\n",
        "\n",
        "Key Benefits:\n",
        "Faster Execution â€“ Converts eager mode operations into a computational graph for speed.\n",
        "Automatic Graph Optimization â€“ TensorFlow applies optimizations like kernel fusion and constant folding.\n",
        "Device Agnostic â€“ Works on both CPU and GPU seamlessly.\n",
        "Improves Deployment â€“ Allows saving and exporting models efficiently.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "98rInjlLObHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "@tf.function\n",
        "def matmul(a, b):\n",
        "    return tf.matmul(a, b)\n",
        "\n",
        "# Create random tensors\n",
        "a = tf.random.normal((1000, 1000))\n",
        "b = tf.random.normal((1000, 1000))\n",
        "\n",
        "# Without tf.function (Eager Execution)\n",
        "start = time.time()\n",
        "result_eager = tf.matmul(a, b)\n",
        "print(\"Eager Execution Time:\", time.time() - start)\n",
        "\n",
        "# With tf.function (Graph Execution)\n",
        "start = time.time()\n",
        "result_graph = matmul(a, b)\n",
        "print(\"Graph Execution Time:\", time.time() - start)\n"
      ],
      "metadata": {
        "id": "uFAhyKs1QCv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. What is the purpose of the Model class in TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "The Model class in TensorFlow 2.0 (part of tf.keras.Model) is used to create custom deep learning models by subclassing it.\n",
        " It provides a flexible way to define architectures, forward passes, and custom training logic.\n",
        "\n",
        "Key Purposes of Model Class:\n",
        "Encapsulates Model Architecture â€“ Allows defining layers in __init__ and forward pass in call().\n",
        "Enhances Flexibility â€“ Enables custom training loops and loss functions.\n",
        "Provides Built-in Methods â€“ Supports .fit(), .evaluate(), and .predict().\n",
        "Easier Saving & Loading â€“ Models can be saved as .h5 or SavedModel formats\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QE2nP_C5ObFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a custom model\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        return self.dense2(x)\n",
        "\n",
        "# Instantiate and compile the model\n",
        "model = MyModel()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Dummy input\n",
        "x = tf.random.normal((1, 10))\n",
        "output = model(x)\n",
        "print(output)  # Forward pass output\n"
      ],
      "metadata": {
        "id": "2VRm5KZ7QWez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. How do you create a neural network using TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "In TensorFlow 2.0, you can create a neural network using the tf.keras.Model API, which provides flexibility for defining custom architectures.\n",
        "The model consists of an input layer, one or more hidden layers with activation functions (e.g., ReLU), and an output layer.\n",
        "The compile() method configures the model with an optimizer, loss function, and evaluation metrics. Training is performed using fit() on a dataset.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9cg34ObZObDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define a neural network using subclassing\n",
        "class MyNeuralNetwork(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyNeuralNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Create and compile the model\n",
        "model = MyNeuralNetwork()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy training data\n",
        "X_train = np.random.rand(100, 10)\n",
        "y_train = np.random.randint(2, size=(100,))\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16)\n"
      ],
      "metadata": {
        "id": "kOfPYTdtQ2oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. What is the importance of Tensor Space in TensorFlow\n",
        "\n",
        "\"\"\"\n",
        "Tensor Space in TensorFlow represents the mathematical structure where tensors exist and operate.\n",
        "It is crucial because TensorFlow is designed to efficiently manipulate tensors (multi-dimensional arrays) for deep learning and numerical computations.\n",
        "\n",
        "Importance of Tensor Space in TensorFlow:\n",
        "Foundation for Computations â€“ All operations (addition, multiplication, gradients) happen in tensor space.\n",
        "Supports High-Dimensional Data â€“ Tensor space allows handling scalars (0D), vectors (1D), matrices (2D), and higher-dimensional tensors (3D+).\n",
        "Optimized for Hardware Acceleration â€“ Tensor operations run efficiently on CPUs, GPUs, and TPUs.\n",
        "Facilitates Deep Learning Models â€“ Weights, activations, and gradients in neural networks exist in tensor space.\n",
        "Enables Automatic Differentiation â€“ TensorFlow computes gradients in tensor space for backpropagation\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yAGs4NJ3ObA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define tensors\n",
        "a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
        "\n",
        "# Perform operations in tensor space\n",
        "c = tf.matmul(a, b)  # Matrix multiplication\n",
        "print(c)\n"
      ],
      "metadata": {
        "id": "_qJejibRRGBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. How can TensorBoard be integrated with TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "TensorBoard is a visualization tool integrated with TensorFlow 2.0 to monitor training metrics,\n",
        " model structure, and more. It is integrated using TensorFlowâ€™s logging and summary features\n",
        "\n",
        " Key Features:\n",
        "Scalars â€“ Monitors loss and accuracy.\n",
        "Graphs â€“ Displays the model architecture.\n",
        "Histograms â€“ Visualizes weight distributions.\n",
        "Hyperparameter Tuning â€“ Helps in performance analysis.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GJbuJaYMOa-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "# Load dataset\n",
        "(X_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "X_train, y_train = X_train / 255.0, y_train  # Normalize\n",
        "\n",
        "# Define a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define TensorBoard callback\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Train model with TensorBoard\n",
        "model.fit(X_train, y_train, epochs=5, callbacks=[tensorboard_callback])\n",
        "\n",
        "# Launch TensorBoard\n",
        "# Run this command in terminal: tensorboard --logdir=logs/fit\n",
        "\n",
        "# After training, run\n",
        "tensorboard --logdir=logs/fit\n",
        "\n"
      ],
      "metadata": {
        "id": "zexwNuQXRbWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. What is the purpose of TensorFlow Playground\n",
        "\n",
        "\"\"\"\n",
        "TensorFlow Playground is an interactive web-based tool that visually demonstrates how neural networks work.\n",
        " It allows users to experiment with different architectures, activation functions, and hyperparameters in a simple and intuitive way.\n",
        "\n",
        "Purpose of TensorFlow Playground:\n",
        "Educational Tool â€“ Helps beginners understand neural networks without coding.\n",
        "Interactive Experimentation â€“ Users can modify layers, neurons, and activations to see real-time effects.\n",
        "Hyperparameter Tuning â€“ Allows adjusting learning rates, batch sizes, and regularization to observe performance changes.\n",
        "Feature Selection â€“ Shows how different input features impact model accuracy.\n",
        "Visualizes Training Process â€“ Displays real-time decision boundaries and loss minimization.\n",
        "How to Use:\n",
        "Visit TensorFlow Playground.\n",
        "Adjust parameters like hidden layers, neurons, learning rate, and activation functions.\n",
        "Click \"Play\" to train and observe how the model learns.\n",
        "Itâ€™s a great tool for beginners to develop intuition about deep learning concepts before diving into TensorFlow coding.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YOhk3xbpOa9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. What is Netron, and how is it useful for deep learning models\n",
        "\n",
        "\"\"\"\n",
        "Netron is an open-source model visualization tool used to inspect and analyze deep learning models.\n",
        " It supports frameworks like TensorFlow, Keras, PyTorch, ONNX, Caffe, and Core ML.\n",
        "\n",
        "How Netron is Useful for Deep Learning Models:\n",
        "Model Structure Visualization â€“ Displays layer connections, tensor shapes, and operations.\n",
        "Debugging & Optimization â€“ Helps identify incorrect layers, missing connections, or inefficient architectures.\n",
        "Framework Compatibility â€“ Supports multiple model formats (.h5, .pb, .onnx, .tflite, etc.).\n",
        "Feature Map Inspection â€“ Shows tensor dimensions at each stage.\n",
        "Lightweight & Interactive â€“ Runs as a desktop app or in a web browser.\n",
        "Netron is especially useful for debugging deep learning models and ensuring correct architecture design before deployment.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m6T8z1eQOa7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q10. What is the difference between TensorFlow and PyTorch\n",
        "\n",
        "\"\"\"\n",
        "1. Ease of Use & Debugging\n",
        "TensorFlow: Uses static computation graphs (tf.function optimizations), requiring explicit graph compilation. Debugging is harder.\n",
        "PyTorch: Uses dynamic computation graphs, making debugging and development more intuitive (eager execution).\n",
        "2. Performance & Deployment\n",
        "TensorFlow: Faster on production with TensorFlow Serving, TensorFlow Lite, and TPU support.\n",
        "PyTorch: Easier for research but now supports deployment with TorchScript and PyTorch Serve.\n",
        "3. Popularity & Community\n",
        "TensorFlow: Preferred in industry and production. Strong support from Google.\n",
        "PyTorch: Dominates academia and research, widely used in universities and AI labs.\n",
        "4. Model Deployment & Mobile Support\n",
        "TensorFlow: Supports TensorFlow Lite for mobile and TensorFlow.js for web.\n",
        "PyTorch: Uses PyTorch Mobile but is less mature than TensorFlowâ€™s ecosystem.\n",
        "5. Visualization Tools\n",
        "TensorFlow: Has TensorBoard for visualization and performance tracking.\n",
        "PyTorch: Uses third-party tools like Weights & Biases and TensorBoardX.\n",
        "6. Syntax & API Design\n",
        "TensorFlow: More structured, requires defining tf.function for performance optimizations.\n",
        "PyTorch: More Pythonic, follows NumPy-like syntax, making it easier for beginners.\n",
        "Which One to Use?\n",
        "-->Choose TensorFlow for production, mobile apps, and large-scale deployment.\n",
        "-->Choose PyTorch for research, prototyping, and dynamic neural network architectures.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p3Ku-2_KOa5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q11. How do you install PyTorch\n",
        "\n",
        "pip install torch torchvision\n",
        "\n",
        "# verify installation\n",
        "import torch\n",
        "print(torch.__version__)  # Check PyTorch version\n",
        "print(torch.cuda.is_available())  # Check if GPU is detected\n"
      ],
      "metadata": {
        "id": "m_3HXYp-Oa3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q12. What is the basic structure of a PyTorch neural network\n",
        "\n",
        "\"\"\"\n",
        "In PyTorch, a neural network is defined as a class that inherits from torch.nn.Module. The structure consists of:\n",
        "\n",
        "__init__() â€“ Defines layers (e.g., nn.Linear, nn.Conv2d).\n",
        "forward() â€“ Specifies how data passes through layers.\n",
        "Optimization & Loss Function â€“ Used for training the model.\n",
        "\n",
        "Key Components\n",
        "nn.Linear() â†’ Defines layers with weights.\n",
        "forward() â†’ Implements forward propagation.\n",
        "loss.backward() â†’ Computes gradients for training.\n",
        "optimizer.step() â†’ Updates weights using gradients.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mulbwXD9Oa1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network\n",
        "class MyNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyNeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer\n",
        "        self.relu = nn.ReLU()  # Activation function\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = MyNeuralNet(input_size=10, hidden_size=5, output_size=1)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Generate dummy data\n",
        "X = torch.randn(10, 10)\n",
        "y = torch.randn(10, 1)\n",
        "\n",
        "# Forward pass, loss computation, and backpropagation\n",
        "output = model(X)\n",
        "loss = criterion(output, y)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "id": "sQ2NLjnaSwfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q13. What is the significance of tensors in PyTorch\n",
        "\n",
        "\"\"\"\n",
        "Tensors are the core data structure in PyTorch, similar to NumPy arrays but optimized for GPU acceleration and automatic differentiation. They enable efficient deep learning computations.\n",
        "\n",
        "Why Tensors Are Important in PyTorch?\n",
        "Efficient Numerical Computation â€“ Tensors support vectorized operations, speeding up matrix calculations.\n",
        "GPU Acceleration â€“ Tensors can be moved to CUDA-enabled GPUs for faster computations.\n",
        "Automatic Differentiation â€“ PyTorchâ€™s torch.autograd tracks tensor operations for backpropagation.\n",
        "Flexible Multi-Dimensional Data â€“ Supports scalars (0D), vectors (1D), matrices (2D), and higher dimensions (3D+ for images, videos, etc.).\n",
        "Interoperability with NumPy â€“ Easily converts between NumPy arrays and PyTorch tensors.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Velecxe6OazL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Creating tensors\n",
        "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "b = torch.randn(2, 2)  # Random tensor\n",
        "\n",
        "# Tensor operations\n",
        "c = a + b  # Addition\n",
        "d = a @ b  # Matrix multiplication\n",
        "\n",
        "# Moving tensor to GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    a = a.to(\"cuda\")\n",
        "\n",
        "print(\"Tensor A:\", a)\n",
        "print(\"Tensor B:\", b)\n",
        "print(\"Sum:\", c)\n",
        "print(\"Matrix Multiplication:\", d)\n"
      ],
      "metadata": {
        "id": "35EFlHhZTB26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch\n",
        "\n",
        "\"\"\"\n",
        "torch.Tensor\n",
        "\n",
        "Stores data on CPU by default.\n",
        "Supports standard PyTorch tensor operations.\n",
        "Slower for large-scale computations compared to GPU tensors.\n",
        "torch.cuda.Tensor\n",
        "\n",
        "Stores data on GPU (CUDA-enabled device).\n",
        "Accelerates matrix operations using NVIDIA CUDA cores.\n",
        "Requires explicit tensor movement using .to(\"cuda\") or .cuda().\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6Fi-ivg0OaxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a CPU tensor (default)\n",
        "cpu_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "print(\"CPU Tensor:\", cpu_tensor.device)  # Output: CPU\n",
        "\n",
        "# Move tensor to GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_tensor = cpu_tensor.to(\"cuda\")  # Convert to CUDA tensor\n",
        "    print(\"GPU Tensor:\", gpu_tensor.device)  # Output: cuda:0\n"
      ],
      "metadata": {
        "id": "gI3EGd9hTKFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q15. What is the purpose of the torch.optim module in PyTorch\n",
        "\n",
        "\"\"\"\n",
        "The torch.optim module provides optimization algorithms for training neural networks by updating model parameters based on computed gradients.\n",
        "\n",
        "Key Features of torch.optim\n",
        "Gradient-Based Optimization â€“ Updates model weights using backpropagation.\n",
        "Supports Multiple Algorithms â€“ Includes SGD, Adam, RMSprop, Adagrad, etc.\n",
        "Learning Rate Scheduling â€“ Adjusts learning rate dynamically using lr_scheduler.\n",
        "Parameter Grouping â€“ Optimizes different layers with different learning rates.\n",
        "Weight Decay (L2 Regularization) â€“ Prevents overfitting by adding penalty terms.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "k7Iyu1bfOau_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple model\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Dummy data\n",
        "X = torch.randn(5, 10)\n",
        "y = torch.randn(5, 1)\n",
        "\n",
        "# Training step\n",
        "optimizer.zero_grad()      # Reset gradients\n",
        "output = model(X)          # Forward pass\n",
        "loss = criterion(output, y)  # Compute loss\n",
        "loss.backward()            # Backpropagation\n",
        "optimizer.step()           # Update model parameters\n",
        "\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "id": "wFRNm3_GTXIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q16. What are some common activation functions used in neural networks"
      ],
      "metadata": {
        "id": "GsRsW5j0Oas2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions introduce non-linearity in neural networks, helping models learn complex patterns.\n",
        "\n",
        "1. ReLU (Rectified Linear Unit)\n",
        "Formula:\n",
        "ð‘“\n",
        "(\n",
        "ð‘¥\n",
        ")\n",
        "=\n",
        "max\n",
        "â¡\n",
        "(\n",
        "0\n",
        ",\n",
        "ð‘¥\n",
        ")\n",
        "f(x)=max(0,x)\n",
        "Pros: Prevents vanishing gradients, computationally efficient.\n",
        "Cons: Can cause \"dying ReLU\" (neurons stuck at 0).\n",
        "Usage: Most common in deep learning (CNNs, MLPs)."
      ],
      "metadata": {
        "id": "k3HEqQfbT0hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sigmoid (Logistic Function)\n",
        "Formula:\n",
        "f(x) = 1 / (1 + exp(-x))\n",
        "Pros: Smooth output between (0,1), good for binary classification.\n",
        "Cons: Causes vanishing gradients, slow convergence.\n",
        "Usage: Used in output layers for binary tasks."
      ],
      "metadata": {
        "id": "EIMlxJcAT7-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tanh (Hyperbolic Tangent)\n",
        "Formula:\n",
        "f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "Pros: Output range (-1,1), better than sigmoid.\n",
        "Cons: Still suffers from vanishing gradients.\n",
        "Usage: Used in RNNs."
      ],
      "metadata": {
        "id": "NXLxl3b_UQCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Leaky ReLU\n",
        "Formula:\n",
        "f(x) = x if x > 0 else 0.01 * x\n",
        "Pros: Prevents dying ReLU problem.\n",
        "Cons: Slightly more computationally expensive.\n",
        "Usage: Used in deep networks."
      ],
      "metadata": {
        "id": "AOdFBxJXURF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Softmax\n",
        "Formula:\n",
        "f(x_i) = exp(x_i) / sum(exp(x_j))\n",
        "Pros: Converts logits into probabilities (sums to 1).\n",
        "Cons: Not used in hidden layers.\n",
        "Usage: Used in multi-class classification (output layer)."
      ],
      "metadata": {
        "id": "pSdyBWKMUT4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch\n",
        "\n",
        "\"\"\"\n",
        "Both torch.nn.Module and torch.nn.Sequential are used to define neural networks, but they have key differences in flexibility and structure.\n",
        "\n",
        "1. torch.nn.Module (Flexible, Custom Models)\n",
        "Used for: Defining complex models with custom layers, forward pass logic, and parameter sharing.\n",
        "Requires: Manually implementing forward() method.\n",
        "Best for: Models with multiple branches, skip connections (e.g., ResNet).\n",
        "\n",
        "2. torch.nn.Sequential (Simpler, Ordered Models)\n",
        "Used for: Creating simple, layer-stacked models without needing a custom forward() method.\n",
        "Requires: Just defining layers in sequence.\n",
        "Best for: Simple feedforward networks without branching logic.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aQsWydRYOaql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using torch.nn.Module\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(10, 20)\n",
        "        self.layer2 = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(20, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "model = MyModel()\n",
        "print(model)\n",
        "\n",
        "\n",
        "# Using torch.nn.Sequential\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 1)\n",
        ")\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "cjeswrMtUoCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q18. How can you monitor training progress in TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "You can monitor training progress in TensorFlow 2.0 using TensorBoard, callbacks, and metrics.\n",
        " TensorBoard provides visualizations for loss, accuracy, and\n",
        "  computational graphs using\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\").\n",
        "  The ModelCheckpoint callback saves the best model during training,\n",
        "  while EarlyStopping stops training if no improvement is detected.\n",
        " You can also print metrics using\n",
        " verbose=1 in model.fit().\n",
        " To use TensorBoard,\n",
        " run %load_ext tensorboard and\n",
        " %tensorboard --logdir logs in Colab\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "06Bi225rOaoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q19. How does the Keras API fit into TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "In TensorFlow 2.0, the Keras API (tf.keras) is the official high-level API, making model building,\n",
        "training, and evaluation simpler. It provides sequential and functional APIs for defining models,\n",
        "integrates seamlessly with TensorFlow's execution (Eager and Graph modes), and supports custom layers,\n",
        "loss functions, and optimizers. tf.keras.Model and tf.keras.layers enable easy model creation,\n",
        "while model.fit(), model.evaluate(), and model.predict() simplify training.\n",
        "Keras also supports callbacks, data pipelines, and distributed training,\n",
        "making it a powerful and user-friendly tool within TensorFlow 2.0.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AYhZSR_NOamF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q20. What is an example of a deep learning project that can be implemented using TensorFlow 2.0\n",
        "\n",
        "\"\"\"\n",
        "Dataset Preparation â€“ Load CIFAR-10 using tf.keras.datasets.cifar10.load_data().\n",
        "Preprocessing â€“ Normalize pixel values to [0,1].\n",
        "Model Building â€“ Create a CNN with tf.keras.Sequential(), using Conv2D, MaxPooling2D, Flatten, Dense, and Dropout layers.\n",
        "Compilation â€“ Compile with Adam optimizer and SparseCategoricalCrossentropy loss.\n",
        "Training â€“ Use model.fit() with training data and validate with test data.\n",
        "Evaluation & Prediction â€“ Test performance with model.evaluate() and predict images using model.predict().\n",
        "Visualization â€“ Monitor training using TensorBoard.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HND8qZ2UOajp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q21. What is the main advantage of using pre-trained models in TensorFlow and PyTorch\n",
        "\n",
        "\"\"\"\n",
        "The main advantage of using pre-trained models in TensorFlow and PyTorch is that\n",
        "they save time and resources by leveraging models already trained on large datasets\n",
        "(e.g., ImageNet). This allows for transfer learning, where you can fine-tune\n",
        "the model on your specific task with less data and computation.\n",
        "Pre-trained models improve accuracy, prevent overfitting,\n",
        " and enable quick deployment in applications like image classification\n",
        "  (ResNet, VGG), NLP (BERT, GPT), and object detection (YOLO, Faster R-CNN)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y6v9ih2WOahQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Practical***"
      ],
      "metadata": {
        "id": "AYP9vTJUPFIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. How do you install and verify that TensorFlow 2.0 was installed successfully"
      ],
      "metadata": {
        "id": "2YoAfzMbOafD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow 2.0\n",
        "pip install tensorflow\n",
        "\n",
        "\n",
        "# Verify Installation\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PHmQq_EyWJRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. How can you define a simple function in TensorFlow 2.0 to perform addition"
      ],
      "metadata": {
        "id": "LXo9KtWMOacq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function  # Optimizes execution\n",
        "def add_numbers(a, b):\n",
        "    return tf.add(a, b)\n",
        "\n",
        "# Example usage\n",
        "result = add_numbers(5, 3)\n",
        "print(result.numpy())  # Output: 8\n"
      ],
      "metadata": {
        "id": "AmwFGQJKWe60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. How can you create a simple neural network in TensorFlow 2.0 with one hidden layer"
      ],
      "metadata": {
        "id": "n9xnN3ojWaPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)),  # Hidden layer (16 neurons)\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer (Binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "_i_CjjbRWtlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.  How can you visualize the training progress using TensorFlow and Matplotlib"
      ],
      "metadata": {
        "id": "BYWUsvBlOaaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample dataset (e.g., binary classification)\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
        "\n",
        "# Define a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and store history\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training & Validation Loss')\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training & Validation Accuracy')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nEWoGBq7W0SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. How do you install PyTorch and verify the PyTorch installation"
      ],
      "metadata": {
        "id": "JxFC9RWlWvdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "xFakHTUAOaYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. How do you create a simple neural network in PyTorch"
      ],
      "metadata": {
        "id": "dOthtz7tOaVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden = nn.Linear(10, 16)  # Hidden layer (16 neurons)\n",
        "        self.output = nn.Linear(16, 1)   # Output layer (1 neuron)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden(x))  # Activation function\n",
        "        x = torch.sigmoid(self.output(x))  # Output activation\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleNN()\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "RLSyR_2BOaS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. How do you define a loss function and optimizer in PyTorch"
      ],
      "metadata": {
        "id": "BlskOQFRXFZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Binary Cross-Entropy Loss (for binary classification)\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Define Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "uUnHp8YQXVD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8.  How do you implement a custom loss function in PyTorch"
      ],
      "metadata": {
        "id": "hFNu4w_QXRHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a Function\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def custom_loss(y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true) ** 2)  # Mean Squared Error (MSE)\n",
        "\n",
        "# Subclassing nn.Module\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return torch.mean((y_pred - y_true) ** 2)  # MSE\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = CustomLoss()\n",
        "\n",
        "# Usage in Training\n",
        "loss = loss_function(y_pred, y_true)\n",
        "\n"
      ],
      "metadata": {
        "id": "2Zc6CDzhXdUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9.  How do you save and load a TensorFlow model\n",
        "\n",
        "\"\"\"\n",
        "We save a trained model in two formats:\n",
        "\n",
        "HDF5 format (.h5)\n",
        "TensorFlow SavedModel format (default)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "92MyA1u8XFSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Save the model (HDF5 format)\n",
        "model.save(\"model.h5\")\n",
        "\n",
        "# Save the model (SavedModel format)\n",
        "model.save(\"saved_model\")\n",
        "\n",
        "\n",
        "# Load the HDF5 model\n",
        "model_h5 = tf.keras.models.load_model(\"model.h5\")\n",
        "\n",
        "# Load the SavedModel format\n",
        "model_tf = tf.keras.models.load_model(\"saved_model\")\n"
      ],
      "metadata": {
        "id": "DQEmu9-pXdv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}